{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTING THE LIBRARIES AND MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local modules\n",
    "import model_helper\n",
    "import preprocess\n",
    "\n",
    "# external libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOADING THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_telecom_customer_data(filename):\n",
    "    if os.path.isfile(filename):\n",
    "      return pd.read_csv(filename)\n",
    "    else:\n",
    "      return (\"Invalid file name, make sure the filename is correct and is in the same package\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TelecomCustomerData = load_telecom_customer_data('TelcoCustomerChurn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TelecomCustomerData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TelecomCustomerData.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORE AND VISUALISE THE DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TelecomCustomerData.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data =TelecomCustomerData, hue=\"Churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=TelecomCustomerData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring and visualising the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy, validate_copy, test_copy = preprocess.process_unencoded_data(data=TelecomCustomerData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_copy, hue='Churn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='Churn', y='MonthlyCharges', data=train_copy, color='lightblue')\n",
    "plt.title(\"Comparing monthly charges of customers that churn and those that do not churn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='TotalCharges', y='Churn', data=train_copy, color='green')\n",
    "plt.title(\"Comparing Total charges of customers that churn and those that do not churn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(data=train_copy,x='Churn',y='TotalCharges' )\n",
    "plt.title(\"Comparing Total charges of customers that churn and those that do not churn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(data=train_copy, x='Churn', y='MonthlyCharges')\n",
    "plt.title(\"Comparing monthly charges of customers that churn and those that do not churn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESS THE DATA\n",
    "Here, I used the function process_unencoded_data in the preprocess file to split data into train, validate, and testing data.\n",
    "Afterwards, I clean the data and prepared it for training, validating and testing by performing the following operations;\n",
    "* Stripped all leading and trailing whitespaces from each categorical column.\n",
    "* Dropped rows where tenure was zero.\n",
    "* Transformed 'TotalCharges' from object data type to float data type.\n",
    "* Dropped duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am going to use the functions defined in the preprocess.py file to prepare the data for training, validating and testin\n",
    "train_data, validate_data, test_data = preprocess.process_clean_data(data=TelecomCustomerData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the control model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the training dataset x and y variables for trianing\n",
    "x_train_data, y_train_data = train_data.drop('Churn', axis=1), train_data.Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the validating dataset into x and y variables for validating\n",
    "x_validate_data, y_validate_data = validate_data.drop('Churn', axis = 1), validate_data.Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the testing dataset into x and y variables for testing\n",
    "x_test_data, y_test_data = test_data.drop('Churn', axis = 1), test_data.Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train_data = sc.fit_transform(x_train_data)\n",
    "x_test_data = sc.transform(x_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the control model score as the baseline using the get_control_score function in the model_helper.py file\n",
    "# I used the DummyClassifier model in the sklearn library for this\n",
    "score = model_helper.get_control_score(x_train_data, y_train_data)\n",
    "print(\"The accuracy for the control model is: \", (score*100).round(2), '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING AND COMPARING MODELS\n",
    "\n",
    "* Here I train models using the Random Forest algorithm from the sklearn library.\n",
    "* The main procedures and computations have been delegated to the functions in the model_helper.py module\n",
    "* I will be using these functions to train and compare different models with the Random Forest classifier by using different random_state, max_depth and min_samples_leaf for each model\n",
    "#### NB: More details in the doctstrings of each function in the model_helper module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifiers = model_helper.compare_models(x_train = x_train_data,y_train= y_train_data, x_validate = x_validate_data,y_validate = y_validate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifiers[random_forest_classifiers['Validate Acc Score'] > 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING AND EVALUATING THE CLASSIFIER WITH THE BEST FEATURE SELECTION AND MAX_DEPTH\n",
    "\n",
    "* Here I mak predictions with the classifier model that appears to perform better after evaluating a number of models with the random forest classifier.\n",
    "* None of the models from the results of the cell above (using different values used for max_depth, and min_samples_leaf in evalating the models ) were grealty over fit\n",
    "* Now, after the evaluation, the model with min_samples_leaf = 5 and max_depth appears to have the best performance sine it has the best recal score and the slightly higher accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now selecting and testing the chosen model.\n",
    "from test_predict import test_classifier\n",
    "model, dataframe = test_classifier(x_train_data, y_train_data,x_validate_data,y_validate_data, x_test_data,y_test_data)\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_fig, ax_f = plt.subplots(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "ax = plt.gca()\n",
    "rfc_disp = RocCurveDisplay.from_estimator(model, x_test_data, y_test_data, ax=ax, alpha=0.8)\n",
    "rfc_disp.plot(ax=ax, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "cm_1 = confusion_matrix(y_test_data, predictions, labels=model.classes_)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm_1,display_labels=model.classes_)\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(x_test_data, y_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_predict import compute_predictions_dataframe   \n",
    "# (explore_data, classifier, X_test):\n",
    "predict_dataframe = compute_predictions_dataframe(test_copy, model, x_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_predict import write_to_csv\n",
    "write_to_csv(predict_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BENCHMARKING WITH KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bench = KNeighborsClassifier(n_neighbors = 5)\n",
    "bench.fit(x_train_data, y_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench.score(x_test_data, y_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = bench.predict(x_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "cm = confusion_matrix(y_test_data, pred, labels=bench.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=bench.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9930ed311909a79bf496311ac9bdcd4a6f8e1da00149b644953bc490107a5c12"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
